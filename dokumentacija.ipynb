{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c879892",
   "metadata": {},
   "source": [
    "# Detekcija spojlera u recenzijama\n",
    "###  Osnove računarske inteligencije\n",
    "**Autor:** Viktor Srbljin (SV63-2022)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826394d4",
   "metadata": {},
   "source": [
    "## 1. Istraživanje\n",
    "Na samom početku rada na projektu sprovedeno je istraživanje različitih pristupa detekciji spoilera u tekstualnim recenzijama filmova i serija. Fokus je bio na poređenju klasičnih i savremenih metoda obrade prirodnog jezika, odnosno kombinaciji statističkih modela i dubokih neuronskih mreža.\n",
    "\n",
    "Prvi modeli koji su mi zapali za oko su jednostavniji modeli Naive Bayes i Logistic Regression.\n",
    "Ovi modeli su idealni za ranu fazu projekta jer su brzi za treniranje, laki za tumačenje i ne zahtevaju veliku količinu računarskih resursa. Takođe, predstavljaju odličnu osnovu za poređenje sa kompleksnijim modelima poput transformera.\n",
    "\n",
    "Nakon uspostavljanja bazne tačke performansi klasičnih modela, istraženi su savremeni transformerski modeli, prvenstveno BERT i njegova lakša varijanta DistilBERT.\n",
    "Za razliku od klasičnih pristupa, ovi modeli razumeju kontekst reči u rečenici, zahvaljujući mehanizmu pažnje (attention mechanism), što ih čini posebno pogodnim za zadatke poput detekcije spoilera - gde značenje zavisi od semantičkih nijansi i odnosa među rečenicama.\n",
    "\n",
    "Međutim, već tokom inicijalne faze eksperimentisanja postalo je jasno da transformerski modeli zahtevaju značajne računarske resurse, posebno GPU akceleraciju.\n",
    "Na lokalnoj mašini (CPU okruženje) treniranje BERT-a ili čak DistilBERT-a bilo je nepraktično zbog dužine trajanja epoha i visokih zahteva za memorijom.\n",
    "Zbog toga je doneta odluka da će se za kasniju fazu projekta koristiti Google Colab ili drugo okruženje sa dostupnim GPU-om, dok će inicijalni razvoj, obrada podataka i evaluacija klasičnih modela biti sprovedeni lokalno."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1a9ac6",
   "metadata": {},
   "source": [
    "## 2. Postavljanje okruženja\n",
    "Okruženje za projekat postavljeno je u Pythonu, uz korišćenje ključnih biblioteka:\n",
    "\n",
    "- pandas, numpy za rad sa podacima,\n",
    "- scikit-learn za implementaciju i evaluaciju klasičnih modela,\n",
    "- nltk i spacy za pretprocesiranje teksta,\n",
    "- transformers i torch za fine-tuning BERT modela.\n",
    "\n",
    "Struktura projekta je organizovana u više foldera (data, src, models, results) kako bi se omogućila modularnost i preglednost eksperimenta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f638ee",
   "metadata": {},
   "source": [
    "## 3. Skup podataka\n",
    "- IMDb Spoiler Dataset ([Misra, 2019 – Kaggle](https://www.kaggle.com/datasets/rmisra/imdb-spoiler-dataset)).\n",
    "- Atributi: `review`, `contains_spoiler`, i dodatni metapodaci (naslov, korisnik, godina...).\n",
    "\n",
    "**Ciljni atribut:** `contains_spoiler`\n",
    "\n",
    "### Analiza podataka\n",
    "Pre treninga modela važno je razumeti dataset kako bismo znali:\n",
    "- koliko ukupno imamo recenzija\n",
    "- da li je dataset uravnotežen po klasama (spoiler / non-spoiler)\n",
    "- prosečnu i raspon dužina recenzija\n",
    "\n",
    "Dataset ima 573,838 recenzija, od kojih ~26% sadrži spoilere, što pokazuje da je dataset neuravnotežen.\n",
    "Prosečna dužina recenzija je oko 132 reči, sa značajnom varijacijom (od 2 do 1309 reči).\n",
    "Dužina recenzija je raznolika, ali većina recenzija ima dovoljno sadržaja za modeliranje.\n",
    "\n",
    "***data_analysis.py***\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "![](results/duzina_recenzija.png)\n",
    "![](results/distribucija_duzine_rec.png)"
   ],
   "id": "206688a98502991f"
  },
  {
   "cell_type": "markdown",
   "id": "cd715ebe",
   "metadata": {},
   "source": [
    "## 4. Pretprocesiranje podataka\n",
    "\n",
    "**Za klasične modele:**\n",
    "- mala slova, uklanjanje html tagova, specijalnih karaktera i brojeva, stopword-ova, kratke reči (slova < 3)\n",
    "- tokenizacija teksta u reči\n",
    "- lematizovanje reči (running -> run)\n",
    "\n",
    "**Za transformatore:**\n",
    "- minimalno čišćenje, uklanja samo html tagove i višak razmaka\n",
    "- Tokenizacija pomoću `AutoTokenizer` iz Hugging Face biblioteke\n",
    "\n",
    "\n",
    "### Priprema dataset-a\n",
    "\n",
    "Uzorkovanje (za transformer):\n",
    "- Radi brzine i memorije, uzima se n=35000 recenzija\n",
    "\n",
    "Rezultat: manji dataset za brže treniranje i evaluaciju.\n",
    "\n",
    "Uravnoteženje klasa:\n",
    "- Ako su spoiler/non-spoiler klase neuravnotežene, manjinska klasa se uzorkuje sa ponavljanjem tako da broj recenzija bude jednak većinskoj klasi\n",
    "\n",
    "Rezultat: model ne favorizuje jednu klasu.\n",
    "\n",
    "\n",
    "***preprocessing.py***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce191acf",
   "metadata": {},
   "source": [
    "## 5. Podela podataka\n",
    "- Train (70%) – koristi se za učenje modela, tj. model „vidi“ ove podatke i prilagođava svoje težine\n",
    "\n",
    "- Validation (15%) – koristi se tokom treniranja da procenimo performanse modela na neviđenim podacima i da odlučimo kada stati ili kako podesiti hiperparametre\n",
    "\n",
    "- Test (15%) – evaluacija na kraju"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc68443",
   "metadata": {},
   "source": [
    "## 6. Klasični modeli\n",
    "### 6.1 TF-IDF vektorizacija\n",
    "\n",
    "Za transformaciju tekstualnih recenzija u numeričke karakteristike koristili smo **TF-IDF (Term Frequency – Inverse Document Frequency)**:\n",
    "\n",
    "- **Term Frequency (TF):** koliko puta se reč pojavljuje u dokumentu.\n",
    "- **Inverse Document Frequency (IDF):** smanjuje težinu čestih reči koje se pojavljuju u mnogim dokumentima (npr. \"the\", \"and\").\n",
    "\n",
    "**Zašto TF-IDF?**\n",
    "- Omogućava modelima da identifikuju relevantne reči i fraze koje razlikuju spoilere od non-spoilera.\n",
    "- Bolje od običnog bag-of-words pristupa jer uzima u obzir i važnost reči unutar čitavog korpusa.\n",
    "\n",
    "**Parametri koje smo koristili:**\n",
    "```python\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=50000,  # koristi najčešćih 50k n-grama\n",
    "    ngram_range=(1, 3),  # unigrams, bigrams i trigrams\n",
    "    min_df=3,            # ignoriši reči koje se pojavljuju <3 puta\n",
    "    sublinear_tf=True,   # log-transformacija TF da smanji uticaj vrlo čestih reči\n",
    "    lowercase=True       # uniformnost slova\n",
    ")\n",
    "```\n",
    "\n",
    "Dataset je neuravnotežen (~26% spoiler recenzija).\n",
    "Korišćen je RandomOverSampler da se broj primera manjinske klase (spoiler) izjednači sa većinskom klasom (non-spoiler).\n",
    "Time modeli ne favorizuju većinsku klasu i bolje generalizuju na oba tipa recenzija.\n",
    "\n",
    "\n",
    "### 6.2 Naive Bayes (MultinomialNB)\n",
    "\n",
    "Zašto MultinomialNB?\n",
    "\n",
    "- Idealan je za diskretne karakteristike poput TF-IDF vrednosti.\n",
    "\n",
    "- Brz je za treniranje i jednostavan za interpretaciju.\n",
    "\n",
    "- Pretpostavlja nezavisnost reči unutar dokumenta (bag-of-words pristup).\n",
    "\n",
    "Parametri:\n",
    "\n",
    "U ovom projektu koristili smo default parametre, što je često dovoljno za baznu implementaciju.\n",
    "\n",
    "### 6.3 Logistic Regression\n",
    "\n",
    "Zašto Logistic Regression?\n",
    "\n",
    "- Omogućava balans između složenosti i interpretabilnosti.\n",
    "- Dobro radi sa visokodimenzionalnim podacima poput TF-IDF matrica.\n",
    "\n",
    "Parametri korišćeni u projektu:\n",
    "```\n",
    "lr = LogisticRegression(\n",
    "    max_iter=1000,         # više iteracija za konvergenciju na velikim skupovima\n",
    "    class_weight='balanced', # povećava težinu manjinske klase\n",
    "    C=2.0,                 # smanjuje regularizacioni penal (povoljnije za učenje detalja)\n",
    "    solver='liblinear',    # pogodan za male i srednje dataset-e\n",
    "    n_jobs=-1              # koristi sve CPU jezgre za paralelizaciju\n",
    ")\n",
    "```\n",
    "\n",
    "- class_weight='balanced': ključan za neuravnotežen dataset, pomaže da LR ne favorizuje non-spoiler\n",
    "- C=2.0: manja regularizacija omogućava modelu da bolje uklopi signale iz TF-IDF matrice\n",
    "- max_iter=1000: osigurava konvergenciju\n",
    "- solver='liblinear': stabilan za male/umerene dataset-e i binarne klasifikacije\n",
    "\n",
    "\n",
    "***KOD MOŽETE NAĆI U***\n",
    "- classical.py\n",
    "- train_all_classical.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054e5cb2",
   "metadata": {},
   "source": [
    "## 7. Transformer modeli\n",
    "Za razliku od klasičnih modela, transformeri poput BERT, DistilBERT i RoBERTa omogućavaju modelu da razume kontekst reči u okviru rečenice ili cele recenzije zahvaljujući mehanizmu pažnje (attention mechanism). Ovo je posebno važno za detekciju spoilera, gde značenje rečenice često zavisi od odnosa između više reči ili rečenica\n",
    "\n",
    "### 7.1 Izbor modela i uzorak podataka\n",
    "Za inicijalno eksperimentisanje, testirani su DistilBERT, BERT i RoBERTa na uzorku od 5000 recenzija, radi brzine treniranja i memorijskih ograničenja (Google Colab sa T4 GPU i sesijama od 12h). Rezultati su bili slični po F1-score-u, pa je odlučeno da se u daljem radu koristi DistilBERT zbog:\n",
    "- manjih zahteva za GPU memoriju,\n",
    "- bržeg treniranja,\n",
    "- lakšeg fine-tuninga nad većim datasetom.\n",
    "\n",
    "Na kraju je fine-tuning sproveden na 35000 recenzija, kako bi se dobio robusniji model sa boljom generalizacijom.\n",
    "\n",
    "### 7.2 Trening modela\n",
    "Za svaki od transformera korišćeni su sledeći parametri:\n",
    "```\n",
    "Model\tLearning Rate\tBatch Size\tEpochs\tMax Length\n",
    "DistilBERT\t2e-5\t       16\t       3\t256\n",
    "BERT\t    3e-5\t       8\t       3\t256\n",
    "RoBERTa\t    2e-5\t       8\t       3\t256\n",
    "```\n",
    "- Learning rate: mala vrednost kako se težine pred-treniranog modela ne bi previše menjale, što omogućava stabilan fine-tuning\n",
    "- Batch size: ograničen GPU memorijom; DistilBERT je mogao da koristi veći batch size, što ubrzava treniranje\n",
    "- Max length: 256 tokena pokriva većinu recenzija, a ne preopterećuje memoriju\n",
    "- Epochs: 3 epohe su dovoljne da model nauči specifičnosti spoilera bez pretreniranja\n",
    "\n",
    "Trening se obavlja pomoću Hugging Face Trainer klase, koja automatski vodi evidenciju o metriki (accuracy, precision, recall, F1) i čuva najbolji model.\n",
    "\n",
    "Sva tri modela (DistilBERT, BERT, RoBERTa) su davala slične performanse nad uzorkom od 5000 recenzija. Zbog ograničenja GPU-a i jednostavnosti, odlučeno je da se nastavi sa DistilBERT za treniranje nad većim skupom od 35000 recenzija.\n",
    "\n",
    "### Komentar\n",
    "Ključna stvar prilikom izrade projekta jeste shvatanje razlika između Bert modela. Pokazali su slične performanse zbog:\n",
    "1. Mali uzorak - 5000 recenzija je relativno mali dataset za fine-tuning transformera\n",
    "2. Slične arhitekture:\n",
    "- BERT je standardni pre-trenirani model sa 12 slojeva i mehanizmom pažnje koji uči kontekstualne reprezentacije reči\n",
    "- DistilBERT je kompresovana verzija BERT-a (manje slojeva, ~40% brži i lakši), ali zadržava većinu performansi\n",
    "- RoBERTa je BERT treniran na većem korpusu i sa drugačijim hiperparametrima pre-treninga, ali osnovni princip arhitekture je isti\n",
    "3. Detekcija spoilera je binarni zadatak, a sa kratkim tekstovima i uniformno pripremljenim recenzijama, svi modeli mogu relativno lako da nauče ključne obrasce i ključne fraze koje signaliziraju spoiler\n",
    "\n",
    "\n",
    "\n",
    "***KOD MOŽETE NAĆI U***\n",
    "- transformers_model.py\n",
    "- train_all_transformers.py\n",
    "- transformers_config.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03f8ca7",
   "metadata": {},
   "source": [
    "## 8. Evaluacija modela\n",
    "Nakon treniranja, modeli su evaluirani na test skupu i dobijeni su sledeći rezultati:\n",
    "\n",
    "**Naive Bayes**\n",
    "\n",
    "| Klasa            | Precision | Recall | F1-score | Support |\n",
    "| ---------------- | --------- | ------ | -------- | ------- |\n",
    "| False            | 0.84      | 0.70   | 0.76     | 63438   |\n",
    "| True             | 0.43      | 0.64   | 0.51     | 22638   |\n",
    "| **Accuracy**     |           |        | 0.68     | 86076   |\n",
    "| **Macro avg**    | 0.64      | 0.67   | 0.64     | 86076   |\n",
    "| **Weighted avg** | 0.74      | 0.68   | 0.70     | 86076   |\n",
    "\n",
    "![](results/confusion_matrix_nb.png)\n",
    "\n",
    "Naive Bayes dobro klasifikuje non-spoiler recenzije, ali slabije prepoznaje spoilere. Ovo je posledica bag-of-words pretpostavke i nezavisnosti reči u dokumentu.\n",
    "\n",
    "**Logistic Regression**\n",
    "\n",
    "| Klasa            | Precision | Recall | F1-score | Support |\n",
    "| ---------------- | --------- | ------ | -------- | ------- |\n",
    "| False            | 0.85      | 0.74   | 0.79     | 63438   |\n",
    "| True             | 0.47      | 0.62   | 0.53     | 22638   |\n",
    "| **Accuracy**     |           |        | 0.71     | 86076   |\n",
    "| **Macro avg**    | 0.66      | 0.68   | 0.66     | 86076   |\n",
    "| **Weighted avg** | 0.75      | 0.71   | 0.72     | 86076   |\n",
    "\n",
    "![](results/confusion_matrix_logres.png)\n",
    "\n",
    "Logistic Regression daje bolje performanse od Naive Bayes-a, naročito u preciznosti i F1-score klasa, zahvaljujući linearnoj kombinaciji TF-IDF karakteristika i balansiranju klasa. Ipak, i dalje ne dostiže kvalitet transformerskog modela.\n",
    "\n",
    "**DistilBERT**\n",
    "\n",
    "| Klasa            | Precision | Recall | F1-score | Support |\n",
    "| ---------------- | --------- | ------ | -------- | ------- |\n",
    "| False            | 0.96      | 0.91   | 0.93     | 3891    |\n",
    "| True             | 0.77      | 0.89   | 0.83     | 1359    |\n",
    "| **Accuracy**     |           |        | 0.90     | 5250    |\n",
    "| **Macro avg**    | 0.86      | 0.90   | 0.88     | 5250    |\n",
    "| **Weighted avg** | 0.91      | 0.90   | 0.90     | 5250    |\n",
    "\n",
    "![](results/confusion_matrix_distilbert.png)\n",
    "\n",
    "Transformer model pokazuje znatno bolje performanse u odnosu na klasične modele, naročito u prepoznavanju spoilera. Ovo je očekivano, jer BERT-based modeli razumeju kontekst i semantičke nijanse rečenica, što je ključno za detekciju spoilera. Iako klasa True ima manji broj primera, recall je visok (0.89), što znači da model uspešno prepoznaje većinu spoilera.\n",
    "\n",
    "\n",
    "***KOD MOŽETE NAĆI U***\n",
    "- evaluate.py\n",
    "- evaluate_all.py"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 9. Analiza rezultata\n",
    "- Diskutuj koje reči/obrasci su najčešće u spoiler recenzijama (npr. „dies“, „turns out“).\n",
    "- Analiziraj greške modela - lažno pozitivne i lažno negativne primere.\n",
    "- Uporedi kontekstualno razumevanje (transformeri) naspram površne statistike (klasični modeli).\n",
    "\n",
    "### 9.1 Poredjenje\n",
    "\n",
    "| Model                        | Accuracy | Precision (True) | Recall (True) | F1 (True) | Glavne karakteristike                                                |\n",
    "| ---------------------------- | -------- | ---------------- | ------------- | --------- | -------------------------------------------------------------------- |\n",
    "| **Naive Bayes**              | 0.68     | 0.43             | 0.64          | 0.51      | Brz, jednostavan, ali sklon greškama u preciznosti.                  |\n",
    "| **Logistic Regression**      | 0.71     | 0.47             | 0.62          | 0.53      | Bolji balans preciznosti i odziva; stabilniji rezultati.             |\n",
    "| **Transformer (DistilBERT)** | 0.90     | 0.77             | 0.89          | 0.83      | Najbolji rezultati, znatno bolja preciznost i razumevanje konteksta. |\n",
    "\n",
    "Naive Bayes\n",
    "- Prednosti: jednostavan, brz i interpretabilan (jasno se vidi uticaj pojedinih reči)\n",
    "- Mane: ima nisku preciznost (0.43) - često klasifikuje negativne instance kao pozitivne\n",
    "- Zaključak: generalizuje lošije jer pretpostavlja nezavisnost reči, što u stvarnim tekstovima nije tačno\n",
    "- F1-score (0.51) ukazuje da nije uravnotežen u predviđanju pozitivne klase\n",
    "\n",
    "\n",
    "Logistic Regression\n",
    "- Poboljšanje u odnosu na Naive Bayes: viši F1-score (ali stabilniji balans između precision i recall)\n",
    "- Preciznost (0.47) i odziv (0.62) su i dalje prilično niski za pozitivnu klasu, ali tačnost raste na 0.71\n",
    "- Model bolje nauči granicu između klasa nego Naive Bayes, jer ne pretpostavlja nezavisnost reči\n",
    "- Interpretabilnost je dobra - lako se vidi koje reči povlače klasu pozitivno ili negativno (iz top 20 reči)\n",
    "\n",
    "DistilBERT\n",
    "- Ubedljivo najbolji rezultati\n",
    "- Model vrlo dobro prepoznaje pozitivne instance i retko pogrešno klasifikuje\n",
    "- Može pravilno tumačiti i duže ili sarkastične rečenice, za razliku od klasičnih modela koji gledaju samo reči\n",
    "\n",
    "### 9.2 ROC i Precision-Recall krive\n",
    "\n",
    "![](results/roc_logreg.png)\n",
    "\n",
    "![](results/roc_nb.png)\n",
    "\n",
    "Vrednost AUC (Area Under Curve) označava ukupnu sposobnost modela da razdvaja klase. Veći AUC znači bolji model.\n",
    "\n",
    "\n",
    "\n",
    "Logistic Regression daje bolje rezultate i pokazuje višu sposobnost razlikovanja pozitivnih i negativnih klasa (AUC=0.773 vs 0.735).\n",
    "Precision-Recall kriva potvrđuje da Logistic Regression održava veću preciznost pri većem recall-u, dok Naive Bayes brže gubi tačnost.\n",
    "Zaključak je da Logistic Regression bolje generalizuje i daje stabilniji učinak pri različitim pragovima odluke, dok je Naive Bayes jednostavniji, ali osetljiviji na promenu praga i pogrešne klasifikacije.\n",
    "\n",
    "\n",
    "### 9.3 Top 20 reči\n",
    " ![](results/top20_logreg.png)\n",
    "\n",
    "![](results/top20_nb.png)\n",
    "\n",
    "***KOD MOŽETE NAĆI U***\n",
    "- result_analysis.py\n",
    "\n",
    "\n",
    "### 9.4 SHAP analiza\n",
    "Pokrenite kod ispod.\n",
    "\n"
   ],
   "id": "a848261bdd8b63d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T11:25:53.601717Z",
     "start_time": "2025-10-17T11:25:53.566268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import webbrowser\n",
    "import os\n",
    "\n",
    "# UTICAJ RECI PRIMER\n",
    "path = os.path.abspath(\"results/shap_explanation.html\")\n",
    "webbrowser.open(f\"file:///{path}\")"
   ],
   "id": "50d73688ef974d6d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "f85f0d26",
   "metadata": {},
   "source": [
    "## 10. Zaključak\n",
    "U ovom projektu su testirana tri različita modela za klasifikaciju sentimenta: Logistic Regression, Naive Bayes i DistilBERT. Rezultati evaluacije pokazuju da su svi modeli uspešno naučili osnovne obrasce u tekstu, ali sa različitim nivoima preciznosti.\n",
    "Na osnovu metrika accuracy, precision, recall i F1-score, može se zaključiti da je DistilBERT ostvario najbolje ukupne rezultate. Iako je bio treniran na značajno manjem skupu podataka (oko 35.000 primera), pokazao je bolju sposobnost razumevanja značenja rečenica i konteksta, za razliku od Logistic Regression i Naive Bayes modela koji se više oslanjaju na površinske obrasce i frekvenciju reči.\n",
    "Razlog za to je činjenica da DistilBERT koristi transformersku arhitekturu koja prepoznaje odnose između reči u kontekstu, dok klasični modeli tekst posmatraju kao skup nezavisnih tokena.\n",
    "\n",
    "Glavno ograničenje projekta je tehničke prirode. Zbog nedostatka NVIDIA grafičke kartice, treniranje je moralo biti sprovedeno u Google Colab okruženju, što je značajno usporilo proces i ograničilo veličinu dataset-a.\n",
    "Zbog toga DistilBERT nije mogao biti treniran na većem broju primera, što bi dodatno poboljšalo rezultate i stabilnost modela.\n",
    "Takođe, tradicionalni modeli bi mogli ostvariti bolje rezultate uz dodatno podešavanje hiperparametara, balansiranje podataka, (GridSearchCV, RandomizedSearchCV), redukcija dimenzionalnosti, veći dataset (možda Rotten Tomatoes)\n",
    "\n",
    "U budućem radu projekat bi se mogao unaprediti na više načina:\n",
    "- Treniranje DistilBERT-a ili većeg BERT modela na celokupnom skupu podataka\n",
    "- Korišćenje drugih transformera kao što su RoBERTa, ALBERT ili XLNet, koji često daju još bolje rezultate u NLP zadacima\n",
    "- Eksperimentisanje sa ensemble pristupom (kombinovanje rezultata više modela)\n",
    "\n",
    "Rad na projektu bio je zahtevan, ali veoma poučan. Najveći izazov predstavljao je rad u ograničenom okruženju bez sopstvene GPU podrške, što je zahtevalo stalno prilagođavanje i optimizaciju rada u Colabu.\n",
    "Mogao bih da kažem da sam naučio dosta jer sam kroz konkretan primer video kako teorijski koncepti funkcionišu u praksi.\n",
    "\n",
    "\n",
    "***MOŽETE POKRENUTI APLIKACIJU SA***   streamlit run app.py u terminalu /src\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0482bd",
   "metadata": {},
   "source": [
    "## 11. Literatura\n",
    "- Misra, R. (2019). IMDb Spoiler Dataset. Kaggle\n",
    "- Jurafsky, D., & Martin, J. H. (2023). *Speech and Language Processing.* Pearson\n",
    "- Hugging Face Transformers dokumentacija\n",
    "- Scikit-learn dokumentacija"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
